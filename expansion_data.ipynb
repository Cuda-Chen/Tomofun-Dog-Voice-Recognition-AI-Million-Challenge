{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbd49ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Load quantized model will cause accuracy drop.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import malaya_speech\n",
    "from malaya_speech import Pipeline\n",
    "import gc\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "def norm_mel(y, sr):\n",
    "    mel = librosa.feature.melspectrogram(y, sr = sr, n_mels = 80,hop_length=512)\n",
    "    mel = librosa.power_to_db(mel, ref=np.max)\n",
    "    return mel.T\n",
    "\n",
    "quantized_model = malaya_speech.vad.deep_model(model = 'vggvox-v2', quantized = True)\n",
    "p = Pipeline()\n",
    "\n",
    "pipeline_left = (\n",
    "    p.map(malaya_speech.generator.frames, frame_duration_ms = 30, sample_rate = 16000)\n",
    ")\n",
    "\n",
    "pipeline_right = (\n",
    "    pipeline_left.batching(5)\n",
    "    .foreach_map(quantized_model.predict)\n",
    "    .flatten()\n",
    ")\n",
    "\n",
    "pipeline_left.foreach_zip(pipeline_right).map(malaya_speech.combine.without_silent,\n",
    "                                             threshold_to_stop = 0.05)\n",
    "\n",
    "\n",
    "def remove_silent(y,sr,time_length=80000):\n",
    "    y =  p(y)['without_silent']\n",
    "    if len(y) > time_length:\n",
    "        y = y[0:0+time_length]\n",
    "    else:\n",
    "        y = np.pad(y, (0,time_length-len(y)))\n",
    "    return y\n",
    "\n",
    "def wav2featuresflow(y, sr):\n",
    "    y_without_silent = remove_silent(y,sr)\n",
    "    melspectrogram = np.rot90(norm_mel(y, sr))\n",
    "    return melspectrogram\n",
    "\n",
    "def preprocessing_X(wav_dir):\n",
    "    files = os.listdir(wav_dir)\n",
    "    files.sort() #正確排序很重要!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    X = torch.FloatTensor([])\n",
    "    for f in tqdm(files):\n",
    "        try:\n",
    "            samples, sample_rate = malaya_speech.load(wav_dir+f)\n",
    "            melspectrogram = wav2featuresflow(samples,sr=sample_rate)\n",
    "            melspectrogram = melspectrogram.reshape(-1,80,157).copy()\n",
    "            melspectrogram = torch.from_numpy(melspectrogram)\n",
    "            X = torch.cat([X,torch.unsqueeze(melspectrogram,0)],dim=0)\n",
    "            gc.collect()\n",
    "        except:\n",
    "            print(f)\n",
    "    print('X shape:',X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c9abf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Electricssound',\n",
       " 'dogbarking',\n",
       " 'expansion_data.ipynb',\n",
       " 'GlassBreakingsound',\n",
       " 'download-from-youtube.sh',\n",
       " 'dogcrying',\n",
       " 'Vaccumsound',\n",
       " 'doghowling',\n",
       " 'dishessound',\n",
       " 'Catsound',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f2a427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.sort(*, key=None, reverse=False)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = os.listdir()\n",
    "dirs.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2797f060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Electricssound',\n",
       " 'dogbarking',\n",
       " 'expansion_data.ipynb',\n",
       " 'GlassBreakingsound',\n",
       " 'download-from-youtube.sh',\n",
       " 'dogcrying',\n",
       " 'Vaccumsound',\n",
       " 'doghowling',\n",
       " 'dishessound',\n",
       " 'Catsound',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42f8d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['Electricssound',\n",
    "        'dogbarking',\n",
    "        'GlassBreakingsound',\n",
    "        'dogcrying',\n",
    "        'Vaccumsound',\n",
    "        'doghowling',\n",
    "        'dishessound',\n",
    "        'Catsound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc624059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A/opt/conda/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=512 is too small for input signal of length=480\n",
      "  n_fft, y.shape[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electricssound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=512 is too small for input signal of length=320\n",
      "  n_fft, y.shape[-1]\n",
      "\n",
      "  5%|▌         | 1/20 [00:09<02:58,  9.42s/it]\u001b[A\n",
      " 10%|█         | 2/20 [00:17<02:32,  8.45s/it]\u001b[A\n",
      " 15%|█▌        | 3/20 [00:25<02:20,  8.26s/it]\u001b[A\n",
      " 20%|██        | 4/20 [00:32<02:08,  8.02s/it]\u001b[A\n",
      " 25%|██▌       | 5/20 [00:39<01:55,  7.67s/it]\u001b[A\n",
      " 30%|███       | 6/20 [00:47<01:46,  7.60s/it]\u001b[A\n",
      " 35%|███▌      | 7/20 [00:54<01:38,  7.55s/it]\u001b[A\n",
      " 40%|████      | 8/20 [01:01<01:28,  7.39s/it]\u001b[A\n",
      " 45%|████▌     | 9/20 [01:09<01:21,  7.41s/it]\u001b[A\n",
      " 50%|█████     | 10/20 [01:16<01:11,  7.19s/it]\u001b[A\n",
      " 55%|█████▌    | 11/20 [01:22<01:03,  7.04s/it]\u001b[A\n",
      " 60%|██████    | 12/20 [01:29<00:56,  7.05s/it]\u001b[A\n",
      " 65%|██████▌   | 13/20 [01:36<00:48,  6.98s/it]\u001b[A\n",
      " 70%|███████   | 14/20 [01:43<00:42,  7.02s/it]\u001b[A\n",
      " 75%|███████▌  | 15/20 [01:51<00:35,  7.19s/it]\u001b[A\n",
      " 80%|████████  | 16/20 [01:58<00:28,  7.17s/it]\u001b[A\n",
      " 85%|████████▌ | 17/20 [02:05<00:21,  7.24s/it]\u001b[A\n",
      " 90%|█████████ | 18/20 [02:13<00:14,  7.24s/it]\u001b[A\n",
      " 95%|█████████▌| 19/20 [02:20<00:07,  7.31s/it]\u001b[A\n",
      "100%|██████████| 20/20 [02:28<00:00,  7.42s/it]\u001b[A\n",
      " 12%|█▎        | 1/8 [02:28<17:18, 148.31s/it]\n",
      "  0%|          | 0/260 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([20, 1, 80, 157])\n",
      "dogbarking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/260 [00:07<30:20,  7.03s/it]\u001b[A\n",
      "  1%|          | 2/260 [00:14<30:37,  7.12s/it]\u001b[A\n",
      "  1%|          | 3/260 [00:21<29:54,  6.98s/it]\u001b[A\n",
      "  2%|▏         | 4/260 [00:28<30:06,  7.06s/it]\u001b[A\n",
      "  2%|▏         | 5/260 [00:36<31:09,  7.33s/it]\u001b[A\n",
      "  2%|▏         | 6/260 [00:42<30:30,  7.21s/it]\u001b[A\n",
      "  3%|▎         | 7/260 [00:50<30:40,  7.28s/it]\u001b[A\n",
      "  3%|▎         | 8/260 [00:57<30:37,  7.29s/it]\u001b[A\n",
      "  3%|▎         | 9/260 [01:05<30:52,  7.38s/it]\u001b[A\n",
      "  4%|▍         | 10/260 [01:12<31:08,  7.47s/it]\u001b[A\n",
      "  4%|▍         | 11/260 [01:20<30:57,  7.46s/it]\u001b[A\n",
      "  5%|▍         | 12/260 [01:28<31:12,  7.55s/it]\u001b[A\n",
      "  5%|▌         | 13/260 [01:36<31:32,  7.66s/it]\u001b[A\n",
      "  5%|▌         | 14/260 [01:43<30:53,  7.54s/it]\u001b[A\n",
      "  6%|▌         | 15/260 [01:47<26:14,  6.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 16/260 [01:47<18:31,  4.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111.wav\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for class_name in tqdm(dirs):\n",
    "    wav_dir = class_name+'/'\n",
    "    print(class_name)\n",
    "    data[class_name] = preprocessing_X(wav_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83707ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(data,'expansion_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a73a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
